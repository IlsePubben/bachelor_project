Requirement already satisfied: matplotlib in /home/s3152383/.local/lib/python3.6/site-packages
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /apps/haswell/software/Python/3.6.4-foss-2019a/lib/python3.6/site-packages/pyparsing-2.2.0-py3.6.egg (from matplotlib)
Requirement already satisfied: numpy>=1.11 in /home/s3152383/.local/lib/python3.6/site-packages (from matplotlib)
Requirement already satisfied: python-dateutil>=2.1 in /apps/haswell/software/Python/3.6.4-foss-2019a/lib/python3.6/site-packages/python_dateutil-2.6.1-py3.6.egg (from matplotlib)
Requirement already satisfied: cycler>=0.10 in /home/s3152383/.local/lib/python3.6/site-packages (from matplotlib)
Requirement already satisfied: kiwisolver>=1.0.1 in /home/s3152383/.local/lib/python3.6/site-packages (from matplotlib)
Requirement already satisfied: six>=1.5 in /home/s3152383/.local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib)
You are using pip version 9.0.1, however version 20.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Requirement already satisfied: keras in /home/s3152383/.local/lib/python3.6/site-packages
Requirement already satisfied: numpy>=1.9.1 in /home/s3152383/.local/lib/python3.6/site-packages (from keras)
Requirement already satisfied: h5py in /home/s3152383/.local/lib/python3.6/site-packages (from keras)
Requirement already satisfied: pyyaml in /home/s3152383/.local/lib/python3.6/site-packages (from keras)
Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/s3152383/.local/lib/python3.6/site-packages (from keras)
Requirement already satisfied: keras-applications>=1.0.6 in /home/s3152383/.local/lib/python3.6/site-packages (from keras)
Requirement already satisfied: scipy>=0.14 in /apps/haswell/software/Python/3.6.4-foss-2019a/lib/python3.6/site-packages/scipy-1.0.0-py3.6-linux-x86_64.egg (from keras)
Requirement already satisfied: six>=1.9.0 in /home/s3152383/.local/lib/python3.6/site-packages (from keras)
You are using pip version 9.0.1, however version 20.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Requirement already up-to-date: tensorflow in /home/s3152383/.local/lib/python3.6/site-packages
Requirement already up-to-date: astor>=0.6.0 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: gast>=0.2.0 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: termcolor>=1.1.0 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: keras-preprocessing>=1.0.5 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: numpy<2.0,>=1.14.5 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: google-pasta>=0.1.6 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: wrapt>=1.11.1 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: wheel>=0.26 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: keras-applications>=1.0.6 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: tensorboard<1.15.0,>=1.14.0 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: grpcio>=1.8.6 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: absl-py>=0.7.0 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Collecting protobuf>=3.6.1 (from tensorflow)
  Downloading https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)
Requirement already up-to-date: six>=1.10.0 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorflow)
Requirement already up-to-date: h5py in /home/s3152383/.local/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow)
Collecting setuptools>=41.0.0 (from tensorboard<1.15.0,>=1.14.0->tensorflow)
  Downloading https://files.pythonhosted.org/packages/db/26/7a2d60129e89914f47423e5b3339f0f50ecc9ef93c0894db5490b40b90ed/setuptools-47.1.0-py3-none-any.whl (583kB)
Requirement already up-to-date: werkzeug>=0.11.15 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow)
Requirement already up-to-date: markdown>=2.6.8 in /home/s3152383/.local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow)
Requirement already up-to-date: importlib-metadata; python_version < "3.8" in /home/s3152383/.local/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow)
Requirement already up-to-date: zipp>=0.5 in /home/s3152383/.local/lib/python3.6/site-packages (from importlib-metadata; python_version < "3.8"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow)
Installing collected packages: setuptools, protobuf
  Found existing installation: setuptools 46.4.0
    Uninstalling setuptools-46.4.0:
      Successfully uninstalled setuptools-46.4.0
  Found existing installation: protobuf 3.12.1
    Uninstalling protobuf-3.12.1:
      Successfully uninstalled protobuf-3.12.1
Successfully installed protobuf-3.12.2 setuptools-47.1.0
You are using pip version 9.0.1, however version 20.1.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
2020-05-28 16:03:41.057715: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-28 16:03:41.687552: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500010000 Hz
2020-05-28 16:03:41.712163: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x54b8ca0 executing computations on platform Host. Devices:
2020-05-28 16:03:41.712253: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-05-28 16:03:41.921833: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
Using TensorFlow backend.
/home/s3152383/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/s3152383/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/s3152383/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Running program with the following parameters:
Algorithm: qva-learning 
Epsilon: 0.05 
Temperature: 0.1 
Visiongrid size: 3 
Discount-factor: 0.99 
Learning rate start: 0.005 
Learning rate end: 0.0005
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
epoch 10000 lr_q: 0.0015811388 lr_v: 0.0015811388 lr_a 0.0015811388
NO LONGER DECREASING EPSILON:  18002 e = -2.7777777672281486e-06
epoch 20000 lr_q: 0.0005 lr_v: 0.0005 lr_a 0.0005
model saved as  outputs/new/qva-learning20000-v3-e0.05-y0.99-lr0.005-lr0.0005.txt


###############################################################################
Peregrine Cluster
Job 11791380 for user 's3152383'
Finished at: Thu May 28 21:03:05 CEST 2020

Job details:
============

Name                : qva_learning
User                : s3152383
Partition           : regular
Nodes               : pg-node015
Cores               : 1
State               : COMPLETED
Submit              : 2020-05-28T16:01:53
Start               : 2020-05-28T16:02:20
End                 : 2020-05-28T21:03:05
Reserved walltime   : 10:00:00
Used walltime       : 05:00:45
Used CPU time       : 04:59:11 (efficiency: 99.48%)
% User (Computation): 97.91%
% System (I/O)      :  2.09%
Mem reserved        : 800M/node
Max Mem used        : 167.76M (pg-node015)
Max Disk Write      : 32.70M (pg-node015)
Max Disk Read       : 50.33M (pg-node015)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
